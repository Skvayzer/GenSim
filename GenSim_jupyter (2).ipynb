{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea83a0f9-8abd-4eaf-9b28-8299b36da8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from cliport import tasks\n",
    "from cliport.dataset import RavensDataset\n",
    "from cliport.environments.environment import Environment\n",
    "import IPython\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from cliport.dataset import RavensDataset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a328b1b7-4d9a-4a41-8462-517aa7fddabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "cfg['n'] = 1000\n",
    "cfg['task'] = 'build-car'\n",
    "cfg['mode'] = 'test'\n",
    "cfg['save_data'] = True\n",
    "cfg['assets_root'] = '/home/cosmos/VScode Projects/coglab/GenSim/cliport/environments/assets'\n",
    "cfg['data_dir'] = '/home/cosmos/VScode Projects/coglab/GenSim/data'\n",
    "cfg['disp'] = False\n",
    "cfg['shared_memory'] = False\n",
    "cfg['record'] = {}\n",
    "cfg['record']['save_video'] = True\n",
    "cfg['record']['save_video_path'] = '/home/cosmos/VScode Projects/coglab/GenSim/videos'\n",
    "cfg['record']['add_text'] = False\n",
    "cfg['record']['add_task_text'] = True\n",
    "cfg['record']['fps'] = 20\n",
    "cfg['record']['video_height'] = 640\n",
    "cfg['record']['video_width'] = 720\n",
    "\n",
    "cfg['dataset'] = {}\n",
    "cfg['dataset']['type'] = 'single' # 'single' or 'multi'\n",
    "cfg['dataset']['images'] = True\n",
    "cfg['dataset']['cache'] = True # load episodes to memory instead of reading from disk\n",
    "cfg['dataset']['augment'] = {}\n",
    "cfg['dataset']['augment']['theta_sigma'] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f430e7-9e9b-4423-b9fa-6e798b850b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text argument:/home/cosmos/VScode Projects/coglab/GenSim/cliport/environments/assets\n",
      "int args: ["
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "        cfg['assets_root'],\n",
    "        disp=cfg['disp'],\n",
    "        shared_memory=cfg['shared_memory'],\n",
    "        hz=480,\n",
    "        record_cfg=cfg['record']\n",
    "    )\n",
    "cfg['task'] = cfg['task'].replace(\"_\", \"-\")\n",
    "task = tasks.names[cfg['task']]()\n",
    "task.mode = cfg['mode']\n",
    "record = cfg['record']['save_video']\n",
    "save_data = cfg['save_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cad3bc0-67eb-4e87-9b2c-5b5bab049489",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = task.oracle(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64169da2-3db1-4fc8-a1de-e9e683613609",
   "metadata": {},
   "source": [
    "# Run ORACLE baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b3c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libx264 @ 0x6017980] -qscale is ignored, -crf is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object 1 match with target 1 rew: 0.167\n",
      "Total Reward: 0.167 | Done: False | Goal: Firstly, create the base of the car by positioning two red blocks side by side.\n",
      "object 0 match with target 0 rew: 0.167\n",
      "object 1 match with target 1 rew: 0.333\n",
      "Total Reward: 0.333 | Done: False | Goal: Firstly, create the base of the car by positioning two red blocks side by side.\n",
      "Total Reward: 0.333 | Done: False | Goal: Then, add the car body by stacking a blue block on top of the base.\n",
      "Total Reward: 0.333 | Done: False | Goal: Then, add the car body by stacking a blue block on top of the base.\n",
      "Total Reward: 0.333 | Done: False | Goal: Then, add the car body by stacking a blue block on top of the base.\n",
      "object 0 match with target 0 rew: 0.333\n",
      "Total Reward: 0.667 | Done: False | Goal: Then, add the car body by stacking a blue block on top of the base.\n",
      "object 2 match with target 2 rew: 0.083\n",
      "Total Reward: 0.750 | Done: False | Goal: For the wheels, place a black cylinder on each side of the base blocks.\n",
      "object 1 match with target 3 rew: 0.083\n",
      "object 2 match with target 2 rew: 0.167\n",
      "Total Reward: 0.833 | Done: False | Goal: For the wheels, place a black cylinder on each side of the base blocks.\n",
      "object 1 match with target 3 rew: 0.083\n",
      "object 2 match with target 2 rew: 0.167\n",
      "object 3 match with target 0 rew: 0.250\n",
      "Total Reward: 0.917 | Done: False | Goal: For the wheels, place a black cylinder on each side of the base blocks.\n",
      "object 0 match with target 1 rew: 0.083\n",
      "object 1 match with target 3 rew: 0.167\n",
      "object 2 match with target 2 rew: 0.250\n",
      "object 3 match with target 0 rew: 0.333\n",
      "Total Reward: 1.000 | Done: True | Goal: For the wheels, place a black cylinder on each side of the base blocks.\n"
     ]
    }
   ],
   "source": [
    "episode_id = 0\n",
    "\n",
    "np.random.seed(episode_id)\n",
    "random.seed(episode_id)\n",
    "\n",
    "env.set_task(task)\n",
    "obs = env.reset()\n",
    "info = env.info\n",
    "reward = 0\n",
    "episode, total_reward = [], 0\n",
    "data_path = os.path.join(cfg['data_dir'], \"{}-{}\".format(cfg['task'], task.mode))\n",
    "dataset = RavensDataset(data_path, cfg, n_demos=0, augment=False)\n",
    "\n",
    "if record:\n",
    "    env.start_rec(f'{dataset.n_episodes+1:06d}')\n",
    "\n",
    "# Rollout expert policy\n",
    "for _ in range(task.max_steps):\n",
    "    act = agent.act(obs, info)\n",
    "    episode.append((obs, act, reward, info))\n",
    "    lang_goal = info['lang_goal']\n",
    "    obs, reward, done, info = env.step(act)\n",
    "    total_reward += reward\n",
    "    print(f'Total Reward: {total_reward:.3f} | Done: {done} | Goal: {lang_goal}')\n",
    "    if done:\n",
    "        break\n",
    "if record:\n",
    "    env.end_rec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c937441",
   "metadata": {},
   "source": [
    "## Run CLIPORT checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GENSIM_ROOT=$(pwd)\n"
     ]
    }
   ],
   "source": [
    "%env GENSIM_ROOT=$(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcd7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['GENSIM_ROOT'] = os.getcwd()\n",
    "os.environ['CLIPORT_ROOT'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e45c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosmos/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2024 08:20:00\n"
     ]
    }
   ],
   "source": [
    "# set GPU\n",
    "# %env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from cliport import tasks\n",
    "from cliport import agents\n",
    "from cliport.utils import utils\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "from cliport.dataset import RavensDataset\n",
    "from cliport.environments.environment import Environment\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_demos = 1000 # number training demonstrations used to train agent\n",
    "n_eval = 1 # number of evaluation instances\n",
    "mode = 'test' # val or test\n",
    "\n",
    "agent_name = 'cliport'\n",
    "model_task = 'multi-language-conditioned' # multi-task agent conditioned with language goals\n",
    "\n",
    "model_folder = 'cliport_quickstart' # path to pre-trained checkpoint\n",
    "ckpt_name = 'steps=400000-val_loss=0.00014655.ckpt' # name of checkpoint to load\n",
    "\n",
    "draw_grasp_lines = True\n",
    "affordance_heatmap_scale = 30\n",
    "\n",
    "### Uncomment the task you want to evaluate on ###\n",
    "# eval_task = 'align-rope'\n",
    "# eval_task = 'assembling-kits-seq-seen-colors'\n",
    "# eval_task = 'assembling-kits-seq-unseen-colors'\n",
    "# eval_task = 'packing-shapes'\n",
    "# eval_task = 'packing-boxes-pairs-seen-colors'\n",
    "# eval_task = 'packing-boxes-pairs-unseen-colors'\n",
    "# eval_task = 'packing-seen-google-objects-seq'\n",
    "# eval_task = 'packing-unseen-google-objects-seq'\n",
    "# eval_task = 'packing-seen-google-objects-group'\n",
    "# eval_task = 'packing-unseen-google-objects-group'\n",
    "# eval_task = 'put-block-in-bowl-seen-colors'\n",
    "# eval_task = 'put-block-in-bowl-unseen-colors'\n",
    "eval_task = 'stack-block-pyramid-seq-seen-colors'\n",
    "# eval_task = 'stack-block-pyramid-seq-unseen-colors'\n",
    "# eval_task = 'separating-piles-seen-colors'\n",
    "# eval_task = 'separating-piles-unseen-colors'\n",
    "# eval_task = 'towers-of-hanoi-seq-seen-colors'\n",
    "# eval_task = 'towers-of-hanoi-seq-unseen-colors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa913150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested training on 1 demos, but only 0 demos exist in the dataset path: /home/cosmos/VScode Projects/coglab/GenSim/data/stack-block-pyramid-seq-seen-colors-test.\n",
      "\n",
      "Eval ID: stack-block-pyramid-seq-seen-colors-cliport-1-0\n",
      "\n",
      "Attn FCN - Stream One: plain_resnet_lat, Stream Two: clip_lingunet_lat, Stream Fusion: add\n",
      "Transport FCN - Stream One: plain_resnet_lat, Stream Two: clip_lingunet_lat, Stream Fusion: conv\n",
      "Agent: stack-block-pyramid-seq-seen-colors-cliport-1-0, Logging: False\n",
      "\n",
      "Loading checkpoint: /home/cosmos/VScode Projects/coglab/GenSim/cliport/cliport_quickstart/multi-language-conditioned-cliport-n1000-train/checkpoints/steps=400000-val_loss=0.00014655.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TwoStreamClipLingUNetLatTransporterAgent:\n\tUnexpected key(s) in state_dict: \"attention.attn_stream_one.layer5.0.conv1.weight\", \"attention.attn_stream_one.layer5.0.conv2.weight\", \"attention.attn_stream_one.layer5.0.conv3.weight\", \"attention.attn_stream_one.layer5.0.shortcut.0.weight\", \"attention.attn_stream_one.layer5.1.conv1.weight\", \"attention.attn_stream_one.layer5.1.conv2.weight\", \"attention.attn_stream_one.layer5.1.conv3.weight\", \"attention.attn_stream_one.layer6.0.conv1.weight\", \"attention.attn_stream_one.layer6.0.conv2.weight\", \"attention.attn_stream_one.layer6.0.conv3.weight\", \"attention.attn_stream_one.layer6.0.shortcut.0.weight\", \"attention.attn_stream_one.layer6.1.conv1.weight\", \"attention.attn_stream_one.layer6.1.conv2.weight\", \"attention.attn_stream_one.layer6.1.conv3.weight\", \"attention.attn_stream_two.up1.conv.double_conv.0.weight\", \"attention.attn_stream_two.up1.conv.double_conv.0.bias\", \"attention.attn_stream_two.up1.conv.double_conv.1.weight\", \"attention.attn_stream_two.up1.conv.double_conv.1.bias\", \"attention.attn_stream_two.up1.conv.double_conv.1.running_mean\", \"attention.attn_stream_two.up1.conv.double_conv.1.running_var\", \"attention.attn_stream_two.up1.conv.double_conv.1.num_batches_tracked\", \"attention.attn_stream_two.up1.conv.double_conv.3.weight\", \"attention.attn_stream_two.up1.conv.double_conv.3.bias\", \"attention.attn_stream_two.up1.conv.double_conv.4.weight\", \"attention.attn_stream_two.up1.conv.double_conv.4.bias\", \"attention.attn_stream_two.up1.conv.double_conv.4.running_mean\", \"attention.attn_stream_two.up1.conv.double_conv.4.running_var\", \"attention.attn_stream_two.up1.conv.double_conv.4.num_batches_tracked\", \"attention.attn_stream_two.lat_fusion1.conv.1.weight\", \"attention.attn_stream_two.up2.conv.double_conv.0.weight\", \"attention.attn_stream_two.up2.conv.double_conv.0.bias\", \"attention.attn_stream_two.up2.conv.double_conv.1.weight\", \"attention.attn_stream_two.up2.conv.double_conv.1.bias\", \"attention.attn_stream_two.up2.conv.double_conv.1.running_mean\", \"attention.attn_stream_two.up2.conv.double_conv.1.running_var\", \"attention.attn_stream_two.up2.conv.double_conv.1.num_batches_tracked\", \"attention.attn_stream_two.up2.conv.double_conv.3.weight\", \"attention.attn_stream_two.up2.conv.double_conv.3.bias\", \"attention.attn_stream_two.up2.conv.double_conv.4.weight\", \"attention.attn_stream_two.up2.conv.double_conv.4.bias\", \"attention.attn_stream_two.up2.conv.double_conv.4.running_mean\", \"attention.attn_stream_two.up2.conv.double_conv.4.running_var\", \"attention.attn_stream_two.up2.conv.double_conv.4.num_batches_tracked\", \"attention.attn_stream_two.lat_fusion2.conv.1.weight\", \"transport.key_stream_one.layer5.0.conv1.weight\", \"transport.key_stream_one.layer5.0.conv2.weight\", \"transport.key_stream_one.layer5.0.conv3.weight\", \"transport.key_stream_one.layer5.0.shortcut.0.weight\", \"transport.key_stream_one.layer5.1.conv1.weight\", \"transport.key_stream_one.layer5.1.conv2.weight\", \"transport.key_stream_one.layer5.1.conv3.weight\", \"transport.key_stream_one.layer6.0.conv1.weight\", \"transport.key_stream_one.layer6.0.conv2.weight\", \"transport.key_stream_one.layer6.0.conv3.weight\", \"transport.key_stream_one.layer6.0.shortcut.0.weight\", \"transport.key_stream_one.layer6.1.conv1.weight\", \"transport.key_stream_one.layer6.1.conv2.weight\", \"transport.key_stream_one.layer6.1.conv3.weight\", \"transport.key_stream_two.up1.conv.double_conv.0.weight\", \"transport.key_stream_two.up1.conv.double_conv.0.bias\", \"transport.key_stream_two.up1.conv.double_conv.1.weight\", \"transport.key_stream_two.up1.conv.double_conv.1.bias\", \"transport.key_stream_two.up1.conv.double_conv.1.running_mean\", \"transport.key_stream_two.up1.conv.double_conv.1.running_var\", \"transport.key_stream_two.up1.conv.double_conv.1.num_batches_tracked\", \"transport.key_stream_two.up1.conv.double_conv.3.weight\", \"transport.key_stream_two.up1.conv.double_conv.3.bias\", \"transport.key_stream_two.up1.conv.double_conv.4.weight\", \"transport.key_stream_two.up1.conv.double_conv.4.bias\", \"transport.key_stream_two.up1.conv.double_conv.4.running_mean\", \"transport.key_stream_two.up1.conv.double_conv.4.running_var\", \"transport.key_stream_two.up1.conv.double_conv.4.num_batches_tracked\", \"transport.key_stream_two.lat_fusion1.conv.1.weight\", \"transport.key_stream_two.up2.conv.double_conv.0.weight\", \"transport.key_stream_two.up2.conv.double_conv.0.bias\", \"transport.key_stream_two.up2.conv.double_conv.1.weight\", \"transport.key_stream_two.up2.conv.double_conv.1.bias\", \"transport.key_stream_two.up2.conv.double_conv.1.running_mean\", \"transport.key_stream_two.up2.conv.double_conv.1.running_var\", \"transport.key_stream_two.up2.conv.double_conv.1.num_batches_tracked\", \"transport.key_stream_two.up2.conv.double_conv.3.weight\", \"transport.key_stream_two.up2.conv.double_conv.3.bias\", \"transport.key_stream_two.up2.conv.double_conv.4.weight\", \"transport.key_stream_two.up2.conv.double_conv.4.bias\", \"transport.key_stream_two.up2.conv.double_conv.4.running_mean\", \"transport.key_stream_two.up2.conv.double_conv.4.running_var\", \"transport.key_stream_two.up2.conv.double_conv.4.num_batches_tracked\", \"transport.key_stream_two.lat_fusion2.conv.1.weight\", \"transport.query_stream_one.layer5.0.conv1.weight\", \"transport.query_stream_one.layer5.0.conv2.weight\", \"transport.query_stream_one.layer5.0.conv3.weight\", \"transport.query_stream_one.layer5.0.shortcut.0.weight\", \"transport.query_stream_one.layer5.1.conv1.weight\", \"transport.query_stream_one.layer5.1.conv2.weight\", \"transport.query_stream_one.layer5.1.conv3.weight\", \"transport.query_stream_one.layer6.0.conv1.weight\", \"transport.query_stream_one.layer6.0.conv2.weight\", \"transport.query_stream_one.layer6.0.conv3.weight\", \"transport.query_stream_one.layer6.0.shortcut.0.weight\", \"transport.query_stream_one.layer6.1.conv1.weight\", \"transport.query_stream_one.layer6.1.conv2.weight\", \"transport.query_stream_one.layer6.1.conv3.weight\", \"transport.query_stream_two.up1.conv.double_conv.0.weight\", \"transport.query_stream_two.up1.conv.double_conv.0.bias\", \"transport.query_stream_two.up1.conv.double_conv.1.weight\", \"transport.query_stream_two.up1.conv.double_conv.1.bias\", \"transport.query_stream_two.up1.conv.double_conv.1.running_mean\", \"transport.query_stream_two.up1.conv.double_conv.1.running_var\", \"transport.query_stream_two.up1.conv.double_conv.1.num_batches_tracked\", \"transport.query_stream_two.up1.conv.double_conv.3.weight\", \"transport.query_stream_two.up1.conv.double_conv.3.bias\", \"transport.query_stream_two.up1.conv.double_conv.4.weight\", \"transport.query_stream_two.up1.conv.double_conv.4.bias\", \"transport.query_stream_two.up1.conv.double_conv.4.running_mean\", \"transport.query_stream_two.up1.conv.double_conv.4.running_var\", \"transport.query_stream_two.up1.conv.double_conv.4.num_batches_tracked\", \"transport.query_stream_two.lat_fusion1.conv.1.weight\", \"transport.query_stream_two.up2.conv.double_conv.0.weight\", \"transport.query_stream_two.up2.conv.double_conv.0.bias\", \"transport.query_stream_two.up2.conv.double_conv.1.weight\", \"transport.query_stream_two.up2.conv.double_conv.1.bias\", \"transport.query_stream_two.up2.conv.double_conv.1.running_mean\", \"transport.query_stream_two.up2.conv.double_conv.1.running_var\", \"transport.query_stream_two.up2.conv.double_conv.1.num_batches_tracked\", \"transport.query_stream_two.up2.conv.double_conv.3.weight\", \"transport.query_stream_two.up2.conv.double_conv.3.bias\", \"transport.query_stream_two.up2.conv.double_conv.4.weight\", \"transport.query_stream_two.up2.conv.double_conv.4.bias\", \"transport.query_stream_two.up2.conv.double_conv.4.running_mean\", \"transport.query_stream_two.up2.conv.double_conv.4.running_var\", \"transport.query_stream_two.up2.conv.double_conv.4.num_batches_tracked\", \"transport.query_stream_two.lat_fusion2.conv.1.weight\". \n\tsize mismatch for attention.attn_stream_two.conv1.0.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for transport.key_stream_two.conv1.0.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for transport.query_stream_two.conv1.0.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cosmos/VScode Projects/coglab/GenSim/GenSim_jupyter (2).ipynb Cell 12\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(vcfg[\u001b[39m'\u001b[39m\u001b[39mmodel_path\u001b[39m\u001b[39m'\u001b[39m], ckpt_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mLoading checkpoint: \u001b[39m\u001b[39m{\u001b[39;00mckpt_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m agent\u001b[39m.\u001b[39mload(ckpt_path)\n",
      "File \u001b[0;32m~/VScode Projects/coglab/GenSim/cliport/agents/transporter.py:377\u001b[0m, in \u001b[0;36mTransporterAgent.load\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m, model_path):\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path, map_location\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_type)[\u001b[39m'\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_type)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TwoStreamClipLingUNetLatTransporterAgent:\n\tUnexpected key(s) in state_dict: \"attention.attn_stream_one.layer5.0.conv1.weight\", \"attention.attn_stream_one.layer5.0.conv2.weight\", \"attention.attn_stream_one.layer5.0.conv3.weight\", \"attention.attn_stream_one.layer5.0.shortcut.0.weight\", \"attention.attn_stream_one.layer5.1.conv1.weight\", \"attention.attn_stream_one.layer5.1.conv2.weight\", \"attention.attn_stream_one.layer5.1.conv3.weight\", \"attention.attn_stream_one.layer6.0.conv1.weight\", \"attention.attn_stream_one.layer6.0.conv2.weight\", \"attention.attn_stream_one.layer6.0.conv3.weight\", \"attention.attn_stream_one.layer6.0.shortcut.0.weight\", \"attention.attn_stream_one.layer6.1.conv1.weight\", \"attention.attn_stream_one.layer6.1.conv2.weight\", \"attention.attn_stream_one.layer6.1.conv3.weight\", \"attention.attn_stream_two.up1.conv.double_conv.0.weight\", \"attention.attn_stream_two.up1.conv.double_conv.0.bias\", \"attention.attn_stream_two.up1.conv.double_conv.1.weight\", \"attention.attn_stream_two.up1.conv.double_conv.1.bias\", \"attention.attn_stream_two.up1.conv.double_conv.1.running_mean\", \"attention.attn_stream_two.up1.conv.double_conv.1.running_var\", \"attention.attn_stream_two.up1.conv.double_conv.1.num_batches_tracked\", \"attention.attn_stream_two.up1.conv.double_conv.3.weight\", \"attention.attn_stream_two.up1.conv.double_conv.3.bias\", \"attention.attn_stream_two.up1.conv.double_conv.4.weight\", \"attention.attn_stream_two.up1.conv.double_conv.4.bias\", \"attention.attn_stream_two.up1.conv.double_conv.4.running_mean\", \"attention.attn_stream_two.up1.conv.double_conv.4.running_var\", \"attention.attn_stream_two.up1.conv.double_conv.4.num_batches_tracked\", \"attention.attn_stream_two.lat_fusion1.conv.1.weight\", \"attention.attn_stream_two.up2.conv.double_conv.0.weight\", \"attention.attn_stream_two.up2.conv.double_conv.0.bias\", \"attention.attn_stream_two.up2.conv.double_conv.1.weight\", \"attention.attn_stream_two.up2.conv.double_conv.1.bias\", \"attention.attn_stream_two.up2.conv.double_conv.1.running_mean\", \"attention.attn_stream_two.up2.conv.double_conv.1.running_var\", \"attention.attn_stream_two.up2.conv.double_conv.1.num_batches_tracked\", \"attention.attn_stream_two.up2.conv.double_conv.3.weight\", \"attention.attn_stream_two.up2.conv.double_conv.3.bias\", \"attention.attn_stream_two.up2.conv.double_conv.4.weight\", \"attention.attn_stream_two.up2.conv.double_conv.4.bias\", \"attention.attn_stream_two.up2.conv.double_conv.4.running_mean\", \"attention.attn_stream_two.up2.conv.double_conv.4.running_var\", \"attention.attn_stream_two.up2.conv.double_conv.4.num_batches_tracked\", \"attention.attn_stream_two.lat_fusion2.conv.1.weight\", \"transport.key_stream_one.layer5.0.conv1.weight\", \"transport.key_stream_one.layer5.0.conv2.weight\", \"transport.key_stream_one.layer5.0.conv3.weight\", \"transport.key_stream_one.layer5.0.shortcut.0.weight\", \"transport.key_stream_one.layer5.1.conv1.weight\", \"transport.key_stream_one.layer5.1.conv2.weight\", \"transport.key_stream_one.layer5.1.conv3.weight\", \"transport.key_stream_one.layer6.0.conv1.weight\", \"transport.key_stream_one.layer6.0.conv2.weight\", \"transport.key_stream_one.layer6.0.conv3.weight\", \"transport.key_stream_one.layer6.0.shortcut.0.weight\", \"transport.key_stream_one.layer6.1.conv1.weight\", \"transport.key_stream_one.layer6.1.conv2.weight\", \"transport.key_stream_one.layer6.1.conv3.weight\", \"transport.key_stream_two.up1.conv.double_conv.0.weight\", \"transport.key_stream_two.up1.conv.double_conv.0.bias\", \"transport.key_stream_two.up1.conv.double_conv.1.weight\", \"transport.key_stream_two.up1.conv.double_conv.1.bias\", \"transport.key_stream_two.up1.conv.double_conv.1.running_mean\", \"transport.key_stream_two.up1.conv.double_conv.1.running_var\", \"transport.key_stream_two.up1.conv.double_conv.1.num_batches_tracked\", \"transport.key_stream_two.up1.conv.double_conv.3.weight\", \"transport.key_stream_two.up1.conv.double_conv.3.bias\", \"transport.key_stream_two.up1.conv.double_conv.4.weight\", \"transport.key_stream_two.up1.conv.double_conv.4.bias\", \"transport.key_stream_two.up1.conv.double_conv.4.running_mean\", \"transport.key_stream_two.up1.conv.double_conv.4.running_var\", \"transport.key_stream_two.up1.conv.double_conv.4.num_batches_tracked\", \"transport.key_stream_two.lat_fusion1.conv.1.weight\", \"transport.key_stream_two.up2.conv.double_conv.0.weight\", \"transport.key_stream_two.up2.conv.double_conv.0.bias\", \"transport.key_stream_two.up2.conv.double_conv.1.weight\", \"transport.key_stream_two.up2.conv.double_conv.1.bias\", \"transport.key_stream_two.up2.conv.double_conv.1.running_mean\", \"transport.key_stream_two.up2.conv.double_conv.1.running_var\", \"transport.key_stream_two.up2.conv.double_conv.1.num_batches_tracked\", \"transport.key_stream_two.up2.conv.double_conv.3.weight\", \"transport.key_stream_two.up2.conv.double_conv.3.bias\", \"transport.key_stream_two.up2.conv.double_conv.4.weight\", \"transport.key_stream_two.up2.conv.double_conv.4.bias\", \"transport.key_stream_two.up2.conv.double_conv.4.running_mean\", \"transport.key_stream_two.up2.conv.double_conv.4.running_var\", \"transport.key_stream_two.up2.conv.double_conv.4.num_batches_tracked\", \"transport.key_stream_two.lat_fusion2.conv.1.weight\", \"transport.query_stream_one.layer5.0.conv1.weight\", \"transport.query_stream_one.layer5.0.conv2.weight\", \"transport.query_stream_one.layer5.0.conv3.weight\", \"transport.query_stream_one.layer5.0.shortcut.0.weight\", \"transport.query_stream_one.layer5.1.conv1.weight\", \"transport.query_stream_one.layer5.1.conv2.weight\", \"transport.query_stream_one.layer5.1.conv3.weight\", \"transport.query_stream_one.layer6.0.conv1.weight\", \"transport.query_stream_one.layer6.0.conv2.weight\", \"transport.query_stream_one.layer6.0.conv3.weight\", \"transport.query_stream_one.layer6.0.shortcut.0.weight\", \"transport.query_stream_one.layer6.1.conv1.weight\", \"transport.query_stream_one.layer6.1.conv2.weight\", \"transport.query_stream_one.layer6.1.conv3.weight\", \"transport.query_stream_two.up1.conv.double_conv.0.weight\", \"transport.query_stream_two.up1.conv.double_conv.0.bias\", \"transport.query_stream_two.up1.conv.double_conv.1.weight\", \"transport.query_stream_two.up1.conv.double_conv.1.bias\", \"transport.query_stream_two.up1.conv.double_conv.1.running_mean\", \"transport.query_stream_two.up1.conv.double_conv.1.running_var\", \"transport.query_stream_two.up1.conv.double_conv.1.num_batches_tracked\", \"transport.query_stream_two.up1.conv.double_conv.3.weight\", \"transport.query_stream_two.up1.conv.double_conv.3.bias\", \"transport.query_stream_two.up1.conv.double_conv.4.weight\", \"transport.query_stream_two.up1.conv.double_conv.4.bias\", \"transport.query_stream_two.up1.conv.double_conv.4.running_mean\", \"transport.query_stream_two.up1.conv.double_conv.4.running_var\", \"transport.query_stream_two.up1.conv.double_conv.4.num_batches_tracked\", \"transport.query_stream_two.lat_fusion1.conv.1.weight\", \"transport.query_stream_two.up2.conv.double_conv.0.weight\", \"transport.query_stream_two.up2.conv.double_conv.0.bias\", \"transport.query_stream_two.up2.conv.double_conv.1.weight\", \"transport.query_stream_two.up2.conv.double_conv.1.bias\", \"transport.query_stream_two.up2.conv.double_conv.1.running_mean\", \"transport.query_stream_two.up2.conv.double_conv.1.running_var\", \"transport.query_stream_two.up2.conv.double_conv.1.num_batches_tracked\", \"transport.query_stream_two.up2.conv.double_conv.3.weight\", \"transport.query_stream_two.up2.conv.double_conv.3.bias\", \"transport.query_stream_two.up2.conv.double_conv.4.weight\", \"transport.query_stream_two.up2.conv.double_conv.4.bias\", \"transport.query_stream_two.up2.conv.double_conv.4.running_mean\", \"transport.query_stream_two.up2.conv.double_conv.4.running_var\", \"transport.query_stream_two.up2.conv.double_conv.4.num_batches_tracked\", \"transport.query_stream_two.lat_fusion2.conv.1.weight\". \n\tsize mismatch for attention.attn_stream_two.conv1.0.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for transport.key_stream_two.conv1.0.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for transport.query_stream_two.conv1.0.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3])."
     ]
    }
   ],
   "source": [
    "root_dir = os.environ['GENSIM_ROOT']\n",
    "assets_root = os.path.join(root_dir, 'cliport/environments/assets/')\n",
    "config_file = 'my_eval.yaml' \n",
    "\n",
    "vcfg = utils.load_hydra_config(os.path.join(root_dir, f'cliport/cfg/{config_file}'))\n",
    "vcfg['data_dir'] = os.path.join(root_dir, 'data')\n",
    "vcfg['mode'] = mode\n",
    "\n",
    "vcfg['model_task'] = model_task\n",
    "vcfg['eval_task'] = eval_task\n",
    "vcfg['agent'] = agent_name\n",
    "\n",
    "# Model and training config paths\n",
    "model_path = os.path.join(root_dir, model_folder)\n",
    "# vcfg['train_config'] = f\"{model_path}/cliport/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/.hydra/config.yaml\"\n",
    "# vcfg['model_path'] = f\"{model_path}/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/checkpoints/\"\n",
    "vcfg['root_dir'] = root_dir\n",
    "tcfg = utils.load_hydra_config(vcfg['train_config'])\n",
    "\n",
    "# Load dataset\n",
    "ds = RavensDataset(os.path.join(vcfg['data_dir'], f'{vcfg[\"eval_task\"]}-{vcfg[\"mode\"]}'), \n",
    "                   tcfg, \n",
    "                   n_demos=n_eval,\n",
    "                   augment=False)\n",
    "data_loader = DataLoader(ds, shuffle=False,\n",
    "                    pin_memory=False,\n",
    "                    num_workers=1)\n",
    "\n",
    "eval_run = 0\n",
    "name = '{}-{}-{}-{}'.format(vcfg['eval_task'], vcfg['agent'], n_eval, eval_run)\n",
    "print(f'\\nEval ID: {name}\\n')\n",
    "\n",
    "# Initialize agent\n",
    "utils.set_seed(eval_run, torch=True)\n",
    "agent = agents.names[vcfg['agent']](name, tcfg, data_loader, data_loader)\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = os.path.join(vcfg['model_path'], ckpt_name)\n",
    "print(f'\\nLoading checkpoint: {ckpt_path}')\n",
    "agent.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and task.\n",
    "env = Environment(\n",
    "    assets_root,\n",
    "    disp=False,\n",
    "    shared_memory=False,\n",
    "    hz=480,\n",
    "    record_cfg=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c22ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0\n",
    "num_eval_instances = min(n_eval, ds.n_episodes)\n",
    "\n",
    "for i in range(num_eval_instances):\n",
    "    print(f'\\nEvaluation Instance: {i + 1}/{num_eval_instances}')\n",
    "    \n",
    "    # Load episode\n",
    "    episode, seed = ds.load(i)\n",
    "    goal = episode[-1]\n",
    "    total_reward = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set task\n",
    "    task_name = vcfg['eval_task']\n",
    "    task = tasks.names[task_name]()\n",
    "    task.mode = mode\n",
    "    \n",
    "    # Set environment\n",
    "    env.seed(seed)\n",
    "    env.set_task(task)\n",
    "    obs = env.reset()\n",
    "    info = env.info\n",
    "    reward = 0\n",
    "    \n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    # Rollout\n",
    "    while (step <= task.max_steps) and not done:\n",
    "        print(f\"Step: {step} ({task.max_steps} max)\")\n",
    "        \n",
    "        # Get batch\n",
    "        if step == task.max_steps-1:\n",
    "            batch = ds.process_goal((obs, None, reward, info), perturb_params=None)\n",
    "        else:\n",
    "            batch = ds.process_sample((obs, None, reward, info), augment=False)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(13, 7))\n",
    "        \n",
    "        # Get color and depth inputs\n",
    "        img = batch['img']\n",
    "        img = torch.from_numpy(img)\n",
    "        color = np.uint8(img.detach().cpu().numpy())[:,:,:3]\n",
    "        color = color.transpose(1,0,2)\n",
    "        depth = np.array(img.detach().cpu().numpy())[:,:,3]\n",
    "        depth = depth.transpose(1,0)\n",
    "        \n",
    "        # Display input color\n",
    "        axs[0,0].imshow(color)\n",
    "        axs[0,0].axes.xaxis.set_visible(False)\n",
    "        axs[0,0].axes.yaxis.set_visible(False)\n",
    "        axs[0,0].set_title('Input RGB')\n",
    "        \n",
    "        # Display input depth\n",
    "        axs[0,1].imshow(depth)\n",
    "        axs[0,1].axes.xaxis.set_visible(False)\n",
    "        axs[0,1].axes.yaxis.set_visible(False)        \n",
    "        axs[0,1].set_title('Input Depth')\n",
    "        \n",
    "        # Display predicted pick affordance\n",
    "        axs[1,0].imshow(color)\n",
    "        axs[1,0].axes.xaxis.set_visible(False)\n",
    "        axs[1,0].axes.yaxis.set_visible(False)\n",
    "        axs[1,0].set_title('Pick Affordance')\n",
    "        \n",
    "        # Display predicted place affordance\n",
    "        axs[1,1].imshow(color)\n",
    "        axs[1,1].axes.xaxis.set_visible(False)\n",
    "        axs[1,1].axes.yaxis.set_visible(False)\n",
    "        axs[1,1].set_title('Place Affordance')\n",
    "        \n",
    "        # Get action predictions\n",
    "        l = str(info['lang_goal'])\n",
    "        act = agent.act(obs, info, goal=None)\n",
    "        pick, place = act['pick'], act['place']\n",
    "        \n",
    "        # Visualize pick affordance\n",
    "        pick_inp = {'inp_img': batch['img'], 'lang_goal': l}\n",
    "        pick_conf = agent.attn_forward(pick_inp)\n",
    "        logits = pick_conf.detach().cpu().numpy()\n",
    "\n",
    "        pick_conf = pick_conf.detach().cpu().numpy()\n",
    "        argmax = np.argmax(pick_conf)\n",
    "        argmax = np.unravel_index(argmax, shape=pick_conf.shape)\n",
    "        p0 = argmax[:2]\n",
    "        p0_theta = (argmax[2] * (2 * np.pi / pick_conf.shape[2])) * -1.0\n",
    "    \n",
    "        line_len = 30\n",
    "        pick0 = (pick[0] + line_len/2.0 * np.sin(p0_theta), pick[1] + line_len/2.0 * np.cos(p0_theta))\n",
    "        pick1 = (pick[0] - line_len/2.0 * np.sin(p0_theta), pick[1] - line_len/2.0 * np.cos(p0_theta))\n",
    "\n",
    "        if draw_grasp_lines:\n",
    "            axs[1,0].plot((pick1[0], pick0[0]), (pick1[1], pick0[1]), color='r', linewidth=1)\n",
    "        \n",
    "        # Visualize place affordance\n",
    "        place_inp = {'inp_img': batch['img'], 'p0': pick, 'lang_goal': l}\n",
    "        place_conf = agent.trans_forward(place_inp)\n",
    "\n",
    "        place_conf = place_conf.permute(1, 2, 0)\n",
    "        place_conf = place_conf.detach().cpu().numpy()\n",
    "        argmax = np.argmax(place_conf)\n",
    "        argmax = np.unravel_index(argmax, shape=place_conf.shape)\n",
    "        p1_pix = argmax[:2]\n",
    "        p1_theta = (argmax[2] * (2 * np.pi / place_conf.shape[2]) + p0_theta) * -1.0\n",
    "        \n",
    "        line_len = 30\n",
    "        place0 = (place[0] + line_len/2.0 * np.sin(p1_theta), place[1] + line_len/2.0 * np.cos(p1_theta))\n",
    "        place1 = (place[0] - line_len/2.0 * np.sin(p1_theta), place[1] - line_len/2.0 * np.cos(p1_theta))\n",
    "\n",
    "        if draw_grasp_lines:\n",
    "            axs[1,1].plot((place1[0], place0[0]), (place1[1], place0[1]), color='g', linewidth=1)\n",
    "        \n",
    "        # Overlay affordances on RGB input\n",
    "        pick_logits_disp = np.uint8(logits * 255 * affordance_heatmap_scale).transpose(1,0,2)\n",
    "        place_logits_disp = np.uint8(np.sum(place_conf, axis=2)[:,:,None] * 255 * affordance_heatmap_scale).transpose(1,0,2)    \n",
    "\n",
    "        pick_logits_disp_masked = np.ma.masked_where(pick_logits_disp < 0, pick_logits_disp)\n",
    "        place_logits_disp_masked = np.ma.masked_where(place_logits_disp < 0, place_logits_disp)\n",
    "\n",
    "        axs[1][0].imshow(pick_logits_disp_masked, alpha=0.75)\n",
    "        axs[1][1].imshow(place_logits_disp_masked, cmap='viridis', alpha=0.75)\n",
    "        \n",
    "        print(f\"Lang Goal: {str(info['lang_goal'])}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Act with the predicted actions\n",
    "        obs, reward, done, info = env.step(act)\n",
    "        step += 1\n",
    "        \n",
    "    if done:\n",
    "        print(\"Done. Success.\")\n",
    "    else:\n",
    "        print(\"Max steps reached. Task failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc8e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosmos/.local/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'my_eval': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested training on 100 demos, but only 0 demos exist in the dataset path: /home/cosmos/VScode Projects/coglab/GenSim/data/multi-language-conditioned-test.text argument:/home/cosmos/VScode Projects/coglab/GenSim/cliport/environments/assets/\n",
      "int args: [\n",
      "Save path for results: /home/cosmos/VScode Projects/coglab/GenSim/cliport/cliport_quickstart/multi-language-conditioned-cliport-n1000-train/checkpoints/\n",
      "Evaluating: ['steps=400000-val_loss=0.00014655.ckpt']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/cosmos/VScode Projects/coglab/GenSim/GenSim_jupyter (2).ipynb Cell 10\u001b[0m in \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ckpts_to_eval\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=222'>223</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m     main()\n",
      "\u001b[1;32m/home/cosmos/VScode Projects/coglab/GenSim/GenSim_jupyter (2).ipynb Cell 10\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_run \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vcfg[\u001b[39m'\u001b[39m\u001b[39mn_repeats\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     \u001b[39m# Initialize agent.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     utils\u001b[39m.\u001b[39mset_seed(train_run, torch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     agent \u001b[39m=\u001b[39m agents\u001b[39m.\u001b[39;49mnames[vcfg[\u001b[39m'\u001b[39;49m\u001b[39magent\u001b[39;49m\u001b[39m'\u001b[39;49m]](name, tcfg, \u001b[39mNone\u001b[39;49;00m, ds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39m# Load checkpoint\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/VScode%20Projects/coglab/GenSim/GenSim_jupyter%20%282%29.ipynb#W6sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     agent\u001b[39m.\u001b[39mload(model_file)\n",
      "File \u001b[0;32m~/VScode Projects/coglab/GenSim/cliport/agents/transporter_lang_goal.py:194\u001b[0m, in \u001b[0;36mTwoStreamClipLingUNetLatTransporterAgent.__init__\u001b[0;34m(self, name, cfg, train_ds, test_ds)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, cfg, train_ds, test_ds):\n\u001b[0;32m--> 194\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(name, cfg, train_ds, test_ds)\n",
      "File \u001b[0;32m~/VScode Projects/coglab/GenSim/cliport/agents/transporter_lang_goal.py:20\u001b[0m, in \u001b[0;36mTwoStreamClipLingUNetTransporterAgent.__init__\u001b[0;34m(self, name, cfg, train_ds, test_ds)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, cfg, train_ds, test_ds):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(name, cfg, train_ds, test_ds)\n",
      "File \u001b[0;32m~/VScode Projects/coglab/GenSim/cliport/agents/transporter.py:31\u001b[0m, in \u001b[0;36mTransporterAgent.__init__\u001b[0;34m(self, name, cfg, train_ds, test_ds)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader \u001b[39m=\u001b[39m train_ds\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_loader \u001b[39m=\u001b[39m test_ds\n\u001b[0;32m---> 31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_ds \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39;49mdataset\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_ds \u001b[39m=\u001b[39m test_ds\u001b[39m.\u001b[39mdataset\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "\"\"\"Ravens main training script.\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from cliport import agents\n",
    "from cliport import dataset\n",
    "from cliport import tasks\n",
    "from cliport.utils import utils\n",
    "from cliport.environments.environment import Environment\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load train cfg\n",
    "    # tcfg = utils.load_hydra_config(vcfg['train_config'])\n",
    "    with initialize(config_path='./cliport/cfg', version_base=\"1.2\"):\n",
    "        vcfg = compose(config_name='my_eval')\n",
    "    tcfg = utils.load_hydra_config(vcfg['train_config'])\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize environment and task.\n",
    "    env = Environment(\n",
    "        vcfg['assets_root'],\n",
    "        disp=vcfg['disp'],\n",
    "        shared_memory=vcfg['shared_memory'],\n",
    "        hz=480,\n",
    "        record_cfg=vcfg['record']\n",
    "    )\n",
    "\n",
    "    # Choose eval mode and task.\n",
    "    mode = vcfg['mode']\n",
    "    eval_task = vcfg['eval_task']\n",
    "    if mode not in {'train', 'val', 'test'}:\n",
    "        raise Exception(\"Invalid mode. Valid options: train, val, test\")\n",
    "\n",
    "    # Load eval dataset.\n",
    "    dataset_type = vcfg['type']\n",
    "    if 'multi' in dataset_type:\n",
    "        ds = dataset.RavensMultiTaskDataset(vcfg['data_dir'],\n",
    "                                            tcfg,\n",
    "                                            group=eval_task,\n",
    "                                            mode=mode,\n",
    "                                            n_demos=vcfg['n_demos'],\n",
    "                                            augment=False)\n",
    "    else:\n",
    "        ds = dataset.RavensDataset(os.path.join(vcfg['data_dir'], f\"{eval_task}-{mode}\"),\n",
    "                                   tcfg,\n",
    "                                   n_demos=vcfg['n_demos'],\n",
    "                                   augment=False)\n",
    "\n",
    "    all_results = {}\n",
    "    name = '{}-{}-n{}'.format(eval_task, vcfg['agent'], vcfg['n_demos'])\n",
    "\n",
    "    # Save path for results.\n",
    "    json_name = f\"multi-results-{mode}.json\" if 'multi' in vcfg['model_path'] else f\"results-{mode}.json\"\n",
    "    save_path = vcfg['save_path']\n",
    "    print(f\"Save path for results: {save_path}\")\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_json = os.path.join(save_path, f'{name}-{json_name}')\n",
    "\n",
    "    # Load existing results.\n",
    "    existing_results = {}\n",
    "    if os.path.exists(save_json):\n",
    "        with open(save_json, 'r') as f:\n",
    "            existing_results = json.load(f)\n",
    "\n",
    "    # Make a list of checkpoints to eval.\n",
    "    ckpts_to_eval = list_ckpts_to_eval(vcfg, existing_results)\n",
    "\n",
    "    # Evaluation loop\n",
    "    print(f\"Evaluating: {str(ckpts_to_eval)}\")\n",
    "    for ckpt in ckpts_to_eval:\n",
    "        model_file = os.path.join(vcfg['model_path'], ckpt)\n",
    "\n",
    "        if not os.path.exists(model_file) or not os.path.isfile(model_file):\n",
    "            print(f\"Checkpoint not found: {model_file}\")\n",
    "            continue\n",
    "        elif not vcfg['update_results'] and ckpt in existing_results:\n",
    "            print(f\"Skipping because of existing results for {model_file}.\")\n",
    "            continue\n",
    "\n",
    "        results = []\n",
    "        mean_reward = 0.0\n",
    "\n",
    "        # Run testing for each training run.\n",
    "        for train_run in range(vcfg['n_repeats']):\n",
    "\n",
    "            # Initialize agent.\n",
    "            utils.set_seed(train_run, torch=True)\n",
    "            agent = agents.names[vcfg['agent']](name, tcfg, None, ds)\n",
    "\n",
    "            # Load checkpoint\n",
    "            agent.load(model_file)\n",
    "            print(f\"Loaded: {model_file}\")\n",
    "\n",
    "            record = vcfg['record']['save_video']\n",
    "            n_demos = vcfg['n_demos']\n",
    "\n",
    "            # Run testing and save total rewards with last transition info.\n",
    "            for i in range(0, n_demos):\n",
    "                print(f'Test: {i + 1}/{n_demos}')\n",
    "                episode, seed = ds.load(i)\n",
    "                goal = episode[-1]\n",
    "                total_reward = 0\n",
    "                np.random.seed(seed)\n",
    "\n",
    "                # set task\n",
    "                if 'multi' in dataset_type:\n",
    "                    task_name = ds.get_curr_task()\n",
    "                    task = tasks.names[task_name]()\n",
    "                    print(f'Evaluating on {task_name}')\n",
    "                else:\n",
    "                    task_name = vcfg['eval_task']\n",
    "                    task = tasks.names[task_name]()\n",
    "\n",
    "                task.mode = mode\n",
    "                env.seed(seed)\n",
    "                env.set_task(task)\n",
    "                obs = env.reset()\n",
    "                info = env.info\n",
    "                reward = 0\n",
    "\n",
    "                # Start recording video (NOTE: super slow)\n",
    "                if record:\n",
    "                    video_name = f'{task_name}-{i+1:06d}'\n",
    "                    if 'multi' in vcfg['model_task']:\n",
    "                        video_name = f\"{vcfg['model_task']}-{video_name}\"\n",
    "                    env.start_rec(video_name)\n",
    "\n",
    "                for _ in range(task.max_steps):\n",
    "                    act = agent.act(obs, info, goal)\n",
    "                    lang_goal = info['lang_goal']\n",
    "                    print(f'Lang Goal: {lang_goal}')\n",
    "                    obs, reward, done, info = env.step(act)\n",
    "                    total_reward += reward\n",
    "                    print(f'Total Reward: {total_reward:.3f} | Done: {done}\\n')\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                results.append((total_reward, info))\n",
    "                mean_reward = np.mean([r for r, i in results])\n",
    "                print(f'Mean: {mean_reward} | Task: {task_name} | Ckpt: {ckpt}')\n",
    "\n",
    "                # End recording video\n",
    "                if record:\n",
    "                    env.end_rec()\n",
    "\n",
    "            all_results[ckpt] = {\n",
    "                'episodes': results,\n",
    "                'mean_reward': mean_reward,\n",
    "            }\n",
    "\n",
    "        # Save results in a json file.\n",
    "        if vcfg['save_results']:\n",
    "\n",
    "            # Load existing results\n",
    "            if os.path.exists(save_json):\n",
    "                with open(save_json, 'r') as f:\n",
    "                    existing_results = json.load(f)\n",
    "                existing_results.update(all_results)\n",
    "                all_results = existing_results\n",
    "\n",
    "            with open(save_json, 'w') as f:\n",
    "                json.dump(all_results, f, indent=4)\n",
    "\n",
    "\n",
    "def list_ckpts_to_eval(vcfg, existing_results):\n",
    "    ckpts_to_eval = []\n",
    "\n",
    "    # Just the last.ckpt\n",
    "    if vcfg['checkpoint_type'] == 'last':\n",
    "        last_ckpt = 'last.ckpt'\n",
    "        ckpts_to_eval.append(last_ckpt)\n",
    "\n",
    "    # Validation checkpoints that haven't been already evaluated.\n",
    "    elif vcfg['checkpoint_type'] == 'val_missing':\n",
    "        checkpoints = sorted([c for c in os.listdir(vcfg['model_path']) if \"steps=\" in c])\n",
    "        ckpts_to_eval = [c for c in checkpoints if c not in existing_results]\n",
    "\n",
    "    # Find the best checkpoint from validation and run eval on the test set.\n",
    "    elif vcfg['checkpoint_type'] == 'test_best':\n",
    "        result_jsons = [c for c in os.listdir(vcfg['results_path']) if \"results-val\" in c]\n",
    "        if 'multi' in vcfg['model_task']:\n",
    "            result_jsons = [r for r in result_jsons if \"multi\" in r]\n",
    "        else:\n",
    "            result_jsons = [r for r in result_jsons if \"multi\" not in r]\n",
    "\n",
    "        if len(result_jsons) > 0:\n",
    "            result_json = result_jsons[0]\n",
    "            with open(os.path.join(vcfg['results_path'], result_json), 'r') as f:\n",
    "                eval_res = json.load(f)\n",
    "            best_checkpoint = 'last.ckpt'\n",
    "            best_success = -1.0\n",
    "            for ckpt, res in eval_res.items():\n",
    "                if res['mean_reward'] > best_success:\n",
    "                    best_checkpoint = ckpt\n",
    "                    best_success = res['mean_reward']\n",
    "            print(best_checkpoint)\n",
    "            ckpt = best_checkpoint\n",
    "            ckpts_to_eval.append(ckpt)\n",
    "        else:\n",
    "            print(\"No best val ckpt found. Using last.ckpt\")\n",
    "            ckpt = 'last.ckpt'\n",
    "            ckpts_to_eval.append(ckpt)\n",
    "\n",
    "    # Load a specific checkpoint with a substring e.g: 'steps=10000'\n",
    "    else:\n",
    "        print(f\"Looking for: {vcfg['checkpoint_type']}\")\n",
    "        checkpoints = [c for c in os.listdir(vcfg['model_path']) if vcfg['checkpoint_type'] in c]\n",
    "        checkpoint = checkpoints[0] if len(checkpoints) > 0 else \"\"\n",
    "        ckpt = checkpoint\n",
    "        ckpts_to_eval.append(ckpt)\n",
    "\n",
    "    return ckpts_to_eval\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e28ec66b-0ed9-4f9d-bd25-d5daea265114",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "/home/cosmos/VScode Projects/coglab/GenSim/cliport/cliport_quickstart/multi-language-conditioned-cliport-n1000-train/.hydra/config.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
